# Crime News Scraper - Project Structure

This document provides a comprehensive overview of the project structure and organization.

## ğŸ“ **Root Directory Structure**

```
crime-news-scraper/
â”œâ”€â”€ ğŸ“ src/                     # Main source code
â”œâ”€â”€ ğŸ“ docs/                    # Documentation
â”œâ”€â”€ ğŸ“ scripts/                 # Utility and test scripts
â”œâ”€â”€ ğŸ“ config/                  # Configuration files
â”œâ”€â”€ ğŸ“ output/                  # Generated output files
â”œâ”€â”€ ğŸ“ logs/                    # Application logs
â”œâ”€â”€ ğŸ“„ README.md               # Main project documentation
â”œâ”€â”€ ğŸ“„ requirements.txt        # Python dependencies
â”œâ”€â”€ ğŸ“„ .env.example           # Environment variables template
â””â”€â”€ ğŸ“„ crime_data.db          # SQLite database (generated)
```

## ğŸ—ï¸ **Source Code Structure (`src/`)**

### **Main Modules**

```
src/
â”œâ”€â”€ ğŸ“„ __init__.py             # Package initialization
â”œâ”€â”€ ğŸ“„ main.py                 # Main application entry point
â”œâ”€â”€ ğŸ“„ database.py             # Database operations
â””â”€â”€ ğŸ“„ perplexity_client.py    # Perplexity API client
```

### **Analyzer Module (`src/analyzer/`)**

```
src/analyzer/
â”œâ”€â”€ ğŸ“„ __init__.py             # Module initialization
â”œâ”€â”€ ğŸ“„ analyzer.py             # Main analyzer orchestrator
â”œâ”€â”€ ğŸ“„ analyzer_manual_test.py # Single batch analyzer for testing
â””â”€â”€ ğŸ“„ claude_client.py        # Claude AI API client
```

**Key Features:**
- AI-powered crime incident analysis using Claude AI
- Address validation and enhancement using Perplexity API
- Business impact scoring and risk assessment
- Processing speed: ~10.4 seconds per article
- Address validation: 81.4% success rate

### **Nearby Finder Module (`src/nearby_finder/`)**

```
src/nearby_finder/
â”œâ”€â”€ ğŸ“„ __init__.py             # Module initialization
â”œâ”€â”€ ğŸ“„ finder.py               # Main business discovery orchestrator
â”œâ”€â”€ ğŸ“„ google_client.py        # Google Maps API client
â”œâ”€â”€ ğŸ“„ config.py               # Configuration settings
â”œâ”€â”€ ğŸ“„ example.py              # Usage examples
â””â”€â”€ ğŸ“„ mock_finder.py          # Mock implementation for testing
```

**EXCLUSIVE TARGET FOCUS:**
- ğŸ’ **Jewelry stores** (primary target - highest priority)
- ğŸ† **Sports memorabilia stores** (secondary target)
- ğŸ‘‘ **Luxury goods stores** (secondary target)
- **100% target filtering** - all other business types excluded

### **Scrapers Module (`src/scrapers/`)**

```
src/scrapers/
â”œâ”€â”€ ğŸ“„ __init__.py             # Module initialization
â”œâ”€â”€ ğŸ“„ unified.py              # Unified scraper orchestrator
â”œâ”€â”€ ğŸ“„ base.py                 # Base scraper class
â”œâ”€â”€ ğŸ“ base/                   # Base scraper implementation
â”œâ”€â”€ ğŸ“ jsa/                    # Jewelers Security Alliance scraper
â”œâ”€â”€ ğŸ“ dfw/                    # DFW news scrapers
â”œâ”€â”€ ğŸ“ wfaa/                   # WFAA news scraper
â”œâ”€â”€ ğŸ“ eightnews/              # 8News scraper
â”œâ”€â”€ ğŸ“ reviewjournal/          # Las Vegas Review Journal scraper
â”œâ”€â”€ ğŸ“ nevadacurrent/          # Nevada Current scraper
â””â”€â”€ ğŸ“ newsapi/                # NewsAPI integration
```

**Source-Specific Scrapers:**
Each scraper module contains:
- `scraper.py` - Main scraping logic
- `utils.py` - Utility functions
- `__init__.py` - Module initialization

### **Address Finder Module (`src/address_finder/`)**

```
src/address_finder/
â”œâ”€â”€ ğŸ“„ __init__.py             # Module initialization
â”œâ”€â”€ ğŸ“„ address_extractor.py    # Address extraction logic
â”œâ”€â”€ ğŸ“„ address_inferrer.py     # Address inference algorithms
â”œâ”€â”€ ğŸ“„ address_confirmer.py    # Address validation and confirmation
â”œâ”€â”€ ğŸ“„ enhanced_finder.py      # Enhanced address finding
â”œâ”€â”€ ğŸ“„ text_analyzer.py        # Text analysis for addresses
â””â”€â”€ ğŸ“„ example.py              # Usage examples
```

### **Utilities Module (`src/utils/`)**

```
src/utils/
â”œâ”€â”€ ğŸ“„ __init__.py             # Module initialization
â””â”€â”€ ğŸ“„ logger.py               # Logging utilities and decorators
```

## ğŸ“š **Documentation Structure (`docs/`)**

```
docs/
â”œâ”€â”€ ğŸ“„ API_REFERENCE.md        # Comprehensive API documentation
â”œâ”€â”€ ğŸ“„ CONFIGURATION.md        # Configuration guide
â”œâ”€â”€ ğŸ“„ PROJECT_STRUCTURE.md    # This file - project structure
â”œâ”€â”€ ğŸ“„ deployment_guide.md     # Production deployment guide
â””â”€â”€ ğŸ“„ project_completion_summary.md  # Project completion summary
```

## ğŸ”§ **Scripts Directory (`scripts/`)**

### **Main Workflow Scripts**
- `run_workflow.py` - Complete workflow execution
- `workflow.py` - Alternative workflow runner

### **Component-Specific Scripts**
- `scrape.py` - Scraping operations
- `analyze.py` - Analysis operations
- `nearby.py` - Nearby business finding
- `run_analyzer.py` - Analyzer execution
- `run_analysis_db.py` - Database-based analysis

### **Testing Scripts**
- `test_workflow_limited.py` - Limited workflow testing
- `test_focused_search.py` - Focused search validation
- `test_performance.py` - Performance benchmarking
- `test_enhanced_analyzer.py` - Enhanced analyzer testing
- `test_newsapi.py` - NewsAPI testing
- `test_web_search.py` - Web search testing
- `test_claude.py` - Claude AI testing

### **Utility Scripts**
- `validate_production_data.py` - Production data validation
- `export_analysis_to_csv.py` - Analysis data export
- `create_complete_scrape.py` - Complete scrape file creation
- `rename_output_files.py` - Output file management
- `code_quality_check.py` - Code quality assessment

### **Example Scripts**
- `nearby_finder_example.py` - Nearby finder usage examples
- `address_finder_example.py` - Address finder usage examples

## âš™ï¸ **Configuration Structure (`config/`)**

```
config/
â””â”€â”€ ğŸ“„ nearby_finder.yaml      # Nearby finder configuration
```

## ğŸ“Š **Output Structure (`output/`)**

```
output/
â”œâ”€â”€ ğŸ“ scraping/               # Scraped articles
â”œâ”€â”€ ğŸ“ analysis/               # Analysis results
â””â”€â”€ ğŸ“ nearby/                 # Nearby business data
```

**File Naming Convention:**
- `YYYYMMDD-[type]_[timestamp].csv`
- Example: `20250601-Analysis_143022.csv`

## ğŸ“ **Log Structure (`logs/`)**

```
logs/
â”œâ”€â”€ ğŸ“„ application.log         # Main application logs
â”œâ”€â”€ ğŸ“„ scraper.log            # Scraper-specific logs
â”œâ”€â”€ ğŸ“„ analyzer.log           # Analyzer-specific logs
â””â”€â”€ ğŸ“„ nearby_finder.log      # Nearby finder logs
```

## ğŸ—„ï¸ **Database Schema**

### **Tables:**

1. **`articles`** - Raw scraped news articles
   - Primary key: `id`
   - Unique constraint: `url`
   - Indexes: `date_scraped`, `source`

2. **`analysis_results`** - AI-analyzed crime incident data
   - Primary key: `id`
   - Foreign key: `article_id` â†’ `articles.id`
   - Indexes: `totalScore`, `businessType`

3. **`nearby_businesses`** - Target businesses near incidents
   - Primary key: `id`
   - Foreign key: `analysis_id` â†’ `analysis_results.id`
   - Indexes: `lead_score`, `businessType`, `distance_from_incident`

## ğŸ¯ **Key Design Principles**

### **1. Modular Architecture**
- Clear separation of concerns
- Reusable components
- Easy to test and maintain

### **2. Focused Targeting**
- Exclusive focus on three target business types
- 100% filtering of non-target businesses
- High-quality lead generation (62.2% score â‰¥5)

### **3. Performance Optimization**
- Efficient database operations with proper indexing
- API rate limiting and error handling
- Batch processing for large datasets

### **4. Data Quality**
- Multiple validation layers
- Comprehensive error handling
- Data integrity constraints

### **5. Scalability**
- Database-first approach
- Configurable batch sizes
- Modular scraper architecture

## ğŸ“ˆ **Performance Metrics**

- **Processing Speed**: ~10.4 seconds per article
- **Address Validation**: 81.4% success rate
- **Lead Quality**: 62.2% high-quality leads (score â‰¥5)
- **Target Business Focus**: 100% (jewelry, sports memorabilia, luxury goods only)
- **Data Quality**: 95.1% business naming success rate

## ğŸ”„ **Data Flow**

```
News Sources â†’ Scrapers â†’ Database â†’ Analyzer â†’ Analysis Results
                                                       â†“
Complete Scrape â† Nearby Finder â† Target Businesses â†
```

## ğŸ› ï¸ **Development Workflow**

1. **Code Development**: Implement features in `src/`
2. **Testing**: Use scripts in `scripts/` for testing
3. **Documentation**: Update docs in `docs/`
4. **Configuration**: Modify settings in `config/`
5. **Deployment**: Follow `docs/deployment_guide.md`

## ğŸ“‹ **File Conventions**

### **Python Files**
- PEP 8 compliant code style
- Comprehensive docstrings
- Type hints for all functions
- Proper import organization

### **Documentation**
- Markdown format
- Clear section headers
- Code examples where applicable
- Performance metrics included

### **Configuration**
- YAML format for complex configurations
- Environment variables for sensitive data
- Clear documentation of all options

This structure ensures maintainability, scalability, and ease of development while maintaining focus on our three target business types for maximum lead quality.
